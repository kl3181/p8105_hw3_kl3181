---
title: "p8105_hw3_kl3181"
author: Kelley Lou
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

Load in dataset.
```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles and which are most items from?
```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Making a plot for number of items ordered in each aisle. 
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 1000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Making a plot to show the three most popular items. 
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "pacakaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

Making a table to compare mean hour of the day at which apples and ice cream are ordered.
```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

### Problem 2

##### Load in the dataset and tidy.
```{r}
accel_df = 
  read_csv(
    "./accel_data.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    weekday = if_else(day %in% c("Saturday","Sunday"), "Weekend", "Weekday"),
  ) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    values_to = "activity_count"
  ) %>% 
  separate(activity, into = c("activity", "minute")) %>% 
  select(-activity) %>% 
  mutate(
    minute = as.numeric(minute),
    day = as.factor(day),
    day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  )
```
The resulting dataset has `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. The columns include information for a 63 year old male with a BMI of 25, with CHF who wore an accelerometer for 5 weeks. The data include the week, day of the week, whether it is a weekday or weekend, minute of the day as well as the activity count.

##### Creating a total activity variable
```{r}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  mutate(
    total_activity = round(total_activity, digits = 0)
  ) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
  knitr::kable()
```
We can see that there are specifically two days of lower activity, notably the last two Saturdays in weeks 4 and 5. Similarly, the last two Sundays also had lower activity than previous weeks.It also looks like levels of activity are pretty similar for Tuesdays, Wednesdays and Thursdays with more varying activity for Mondays and Fridays. 

##### Now create a plot of 24 hour time activity.
```{r}
accel_df %>% 
  ggplot(aes(x = minute, y = activity_count)) +
  geom_line(aes(color = day), alpha = 0.3) +
  geom_smooth(size = 0.2) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440),
    labels = c("00:00", "4:00", "8:00", "12:00", "16:00", "20:00", "23:59")
  ) +
  labs(
    title = "Activity levels over 24 hours by day of the week",
    x = "Time of day",
    y = "Level of activity"
  )
```
Based on this graphic, we can see that most of the activity is around 1250, with a few days that have heightened activity. For example, there is more activity on Sunday around 11:00 AM and also more activity in the evenings on Friday. As expected, there is much less activity in the late evening and early morning hours.

### Problem 3

##### Load in the dataset.
```{r}
data("ny_noaa")
```
This dataset consists of five variables for all New York state weather stations from January 1, 1981 through December 31, 2010. There are `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Key variables consist of the weather station ID, observation date, precipitation, snowfall, snow depth, and minimum and maximum temperatures. However, the weather stations do not collect all of these variables, and therefore there is a lot of missing data. 

##### Tidy the dataset.
```{r}
noaa_tidy =
ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day),
    tmin = as.numeric(tmin) / 10,
    tmax = as.numeric(tmax) / 10,
    prcp = prcp / 10
  )
```

Finding the most common snowfall observed values.
```{r}
noaa_tidy %>% 
  count(snow, name = "n_obs") %>% 
  arrange(desc(n_obs))
```
The most common observed value is 0 centimeters of snowfall with 2008508 observations and NA. The most common snowfall is 25 millimieters with 31022. It is likely these are the most common values because for most days out of the year there is no snow or some stations are less likely to have snow than others.

##### Making a two-panel plot to show the average *max* temperature in January and July.
```{r}
noaa_tidy %>%
  group_by(id, year, month) %>%  
  summarize(average_max_temp = mean(tmax, na.rm = TRUE)) %>% 
  filter(month %in% c("1", "7")) %>% 
  ggplot(aes(x = year, y = average_max_temp, group = id)) +
  geom_point(alpha = 0.15, size = 0.5) +
  geom_smooth(se = FALSE) +
  facet_grid(. ~ month) +
  labs(
    title = "Average max temperature over time in January and July",
    x = "Station ID",
    y = "Average max temperature (C)")
```
Looking at the average maximum temperature across the stations, we can see that the maximum temperatures are definitely higher for all the stations in July compared to January. Additionally, There are a few stations where it looks like the maximum temperature difference is greater for certain stations. Over time, it looks like temperatures are fairly consistent, however there seems to be more variation as time goes on.

##### Making a two-panel plot for tmax and tmin for full dataset.
```{r}
library(patchwork)

temperature_p = 
  noaa_tidy %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    title = "Maximum vs minimum temperatures",
    x = "Minimum daily temperature (C)",
    y = "Maximum daily temperature (C)"
  ) +
  theme(plot.title = element_text(size = 10))

snow_p = 
  noaa_tidy %>% 
  filter(snow > 0, snow < 100) %>% 
  mutate(year = factor(year)) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
  labs(
    title = "Distribution of snowfall over time",
    x = "Year",
    y = "Snowfall"
  ) +
  theme(
    plot.title = element_text(size = 10),
    axis.text.x = element_text(angle = 90)
  )  

temperature_p / snow_p
```

  