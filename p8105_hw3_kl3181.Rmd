---
title: "p8105_hw3_kl3181"
author: Kelley Lou
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

Load in dataset.
```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles and which are most items from?
```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Making a plot for number of items ordered in each aisle. 
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 1000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Making a plot to show the three most popular items. 
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "pacakaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

Making a table to compare mean hour of the day at which apples and ice cream are ordered.
```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

### Problem 2

##### Load in the dataset and tidy.
```{r}
accel_df = 
  read_csv(
    "./accel_data.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    weekday = if_else(day %in% c("Saturday","Sunday"), "Weekend", "Weekday"),
  ) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    values_to = "activity_count"
  ) %>% 
  separate(activity, into = c("activity", "minute")) %>% 
  select(-activity) %>% 
  mutate(
    minute = as.numeric(minute),
    day = as.factor(day),
    day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  )
```
The resulting dataset has `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. The columns include information for a 63 year old male with a BMI of 25, with CHF who wore an accelerometer for 5 weeks. The data include the week, day of the week, whether it is a weekday or weekend, minute of the day as well as the activity count.

##### Creating a total activity variable
```{r}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  mutate(
    total_activity = round(total_activity, digits = 0)
  ) %>% 
  knitr::kable()
```
We can see that there are specifically two days of lower activity, notably the last two Saturdays in weeks 4 and 5. Similarly, the last two Sundays also had lower activity than previous weeks.It also looks like levels of activity are pretty similar for Tuesdays, Wednesdays and Thursdays with more varying activity for Mondays and Fridays. 

##### Now create a plot of 24 hour time activity.
```{r}
accel_df %>% 
  ggplot(aes(x = minute, y = activity_count)) +
  geom_line(aes(color = day), alpha = 0.3) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440),
    labels = c("00:00", "4:00", "8:00", "12:00", "16:00", "20:00", "23:59")
  ) +
  labs(
    title = "Activity levels over 24 hours by day of the week",
    x = "Time of day",
    y = "Level of activity"
  )
```
Based on this graphic, we can see that most of the activity is around 1250, with a few days that have heightened activity. For example, there is more activity on Sunday around 11:00 AM and also more activity in the evenings on Friday. As expected, there is much less activity in the late evening and early morning hours.

### Problem 3

##### Load in the dataset.
```{r}
data("ny_noaa")
```
This dataset consists of five variables for all New York state weather stations from January 1, 1981 through December 31, 2010. There are `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Key variables consist of the weather station ID, observation date, precipitation, snowfall, snow depth, and minimum and maximum temperatures. However, the weather stations do not collect all of these variables, and therefore there is a lot of missing data. 

##### Tidy the dataset.
```{r}
```

separate to get three variables year month and day
check in NY NOAA dataset description to see if the unit is reasonable for temp, precipitation, snowfall - convert to degrees (not tenths)
snowfall values (count) 

two panel plot = 
data manipulation
  filter jan and july
  group by station, year, month
  summarize average max temp
ggplot 
  plot time course for station 1, 2, 3, 4, 5
  average max temp over years, separately for each station (TWO PANEL)
  faceting for two panel
  global warming? are some stations always colder than others?
  
two panel plot = 
  two distinct plots, then join them (patchwork)
  tmax vs. tmin
    hex plot 
  snowfall > 0, <100
    filter 
    box plot, violin plot, ridge plot
    one box for each year
  